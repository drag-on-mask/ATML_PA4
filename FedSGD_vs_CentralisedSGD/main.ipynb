{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“‹ Task 1: FedSGD vs. Centralized SGD Equivalence\n",
    "\n",
    "This notebook implements Task 1 from the assignment. The goal is to demonstrate the theoretical equivalence between **Federated SGD (FedSGD)** and **Centralized SGD** under a controlled scenario:\n",
    "\n",
    "1.  **FedSGD:** All clients participate, and each performs a *single SGD step* on its *entire* local dataset (full-batch).\n",
    "2.  **Centralized SGD:** One model is trained with a *single SGD step* on the *entire* global dataset (full-batch).\n",
    "\n",
    "We will use the `SimpleCNN` model from `federated_learning.py` to ensure architectural consistency with Task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported components from federated_learning.py\n",
      "Make sure 'federated_learning.py' is in the same directory.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots appear inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# --- Import from your Task 2 file ---\n",
    "# We reuse the model, aggregation, and evaluation functions\n",
    "# from your 'federated_learning.py' file.\n",
    "try:\n",
    "    from federated_learning import SimpleCNN, aggregate_models, evaluate_model\n",
    "    print(\"Successfully imported components from federated_learning.py\")\n",
    "    print(\"Make sure 'federated_learning.py' is in the same directory.\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: Could not find 'federated_learning.py'.\")\n",
    "    print(\"Please make sure it is in the same directory as this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Task 1 Configuration ---\n",
    "NUM_CLIENTS = 6       # As suggested in assignment\n",
    "NUM_ROUNDS = 20      # Run for 10-20 iterations\n",
    "LEARNING_RATE = 0.01\n",
    "SEED = 42\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_task1(num_clients: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Loads CIFAR-10 and prepares data for Task 1:\n",
    "    1. Centralized Loader: Full training set in one batch.\n",
    "    2. FedSGD Loaders: IID split, each client's loader has one full batch.\n",
    "    3. Test Loader: Standard test set.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # 1. Centralized Loader\n",
    "    centralized_loader = DataLoader(\n",
    "        train_dataset, batch_size=len(train_dataset), shuffle=False\n",
    "    )\n",
    "    print(f\"Centralized loader: 1 batch of {len(train_dataset)} samples.\")\n",
    "\n",
    "    # 2. FedSGD Loaders (IID)\n",
    "    total_size = len(train_dataset)\n",
    "    indices = np.random.permutation(total_size)\n",
    "    # Split evenly\n",
    "    split_size = total_size // num_clients\n",
    "    \n",
    "    fedsgd_loaders = []\n",
    "    client_sizes = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        start_idx = i * split_size\n",
    "        # Assign remainder to the last client\n",
    "        end_idx = (i + 1) * split_size if i < num_clients - 1 else total_size\n",
    "        client_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        client_subset = Subset(train_dataset, client_indices)\n",
    "        \n",
    "        # This is the key change: batch_size = full local dataset size\n",
    "        client_loader = DataLoader(\n",
    "            client_subset, batch_size=len(client_indices), shuffle=False\n",
    "        )\n",
    "        \n",
    "        fedsgd_loaders.append(client_loader)\n",
    "        client_sizes.append(len(client_indices))\n",
    "\n",
    "    print(f\"FedSGD loaders: {num_clients} clients.\")\n",
    "    print(f\"Client sizes: {client_sizes}\")\n",
    "\n",
    "    # Calculate client weights (N_i / N) for aggregation\n",
    "    total_samples = sum(client_sizes)\n",
    "    client_weights = [size / total_samples for size in client_sizes]\n",
    "\n",
    "    # 3. Test Loader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    return centralized_loader, fedsgd_loaders, test_loader, client_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_centralized(model, data_loader, test_loader, lr, num_rounds):\n",
    "    \"\"\"\n",
    "    Trains a model using centralized, full-batch Gradient Descent.\n",
    "    \"\"\"\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history = {'test_acc': [], 'test_loss': [], 'weights': []}\n",
    "    \n",
    "    # Get the single, full batch of data\n",
    "    full_data, full_target = next(iter(data_loader))\n",
    "    full_data, full_target = full_data.to(DEVICE), full_target.to(DEVICE)\n",
    "\n",
    "    print(\"\\n--- Starting Centralized Training ---\")\n",
    "    for round_idx in range(num_rounds):\n",
    "        model.train()\n",
    "        \n",
    "        # Perform one full-batch SGD step\n",
    "        optimizer.zero_grad()\n",
    "        output = model(full_data)\n",
    "        loss = criterion(output, full_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        test_acc, test_loss = evaluate_model(model, test_loader, DEVICE)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['weights'].append(copy.deepcopy(model.state_dict()))\n",
    "        \n",
    "        print(f\"Round {round_idx+1}/{num_rounds} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "    print(\"--- Centralized Training Complete ---\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update_task1(model, data_loader, lr):\n",
    "    \"\"\"\n",
    "    Performs K=1 local step of full-batch GD.\n",
    "    \"\"\"\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # The data loader was created to have only ONE batch\n",
    "    try:\n",
    "        data, target = next(iter(data_loader))\n",
    "    except StopIteration:\n",
    "        print(\"Error: Client data loader is empty.\")\n",
    "        return model.state_dict()\n",
    "        \n",
    "    data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "    # Perform one full-batch SGD step\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return model.state_dict()\n",
    "\n",
    "def train_fedsgd(model, client_loaders, test_loader, client_weights, lr, num_rounds):\n",
    "    \"\"\"\n",
    "    Trains a model using FedSGD.\n",
    "    - K=1 full-batch step per client\n",
    "    - Full client participation\n",
    "    \"\"\"\n",
    "    model.to(DEVICE)\n",
    "    history = {'test_acc': [], 'test_loss': [], 'weights': []}\n",
    "    \n",
    "    print(\"\\n--- Starting FedSGD Training ---\")\n",
    "    for round_idx in range(num_rounds):\n",
    "        client_models_params = []\n",
    "        global_params = model.state_dict()\n",
    "        \n",
    "        # All clients participate in each round\n",
    "        for client_idx in range(len(client_loaders)):\n",
    "            local_model = SimpleCNN() # Use the same imported model class\n",
    "            local_model.load_state_dict(copy.deepcopy(global_params))\n",
    "            \n",
    "            # Perform the single local step\n",
    "            updated_params = client_update_task1(\n",
    "                local_model,\n",
    "                client_loaders[client_idx],\n",
    "                lr=lr\n",
    "            )\n",
    "            client_models_params.append(updated_params)\n",
    "            \n",
    "        # Aggregate the *model parameters*\n",
    "        # Note: Aggregating 1-step models is equivalent to averaging 1-step gradients\n",
    "        aggregated_params = aggregate_models(client_models_params, client_weights)\n",
    "        model.load_state_dict(aggregated_params)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_acc, test_loss = evaluate_model(model, test_loader, DEVICE)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['weights'].append(copy.deepcopy(model.state_dict()))\n",
    "        \n",
    "        print(f\"Round {round_idx+1}/{num_rounds} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "    print(\"--- FedSGD Training Complete ---\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centralized loader: 1 batch of 50000 samples.\n",
      "FedSGD loaders: 6 clients.\n",
      "Client sizes: [8333, 8333, 8333, 8333, 8333, 8335]\n",
      "\n",
      "--- Starting FedSGD Training ---\n",
      "Round 1/20 | Test Loss: 2.3032 | Test Acc: 8.82%\n",
      "Round 2/20 | Test Loss: 2.3012 | Test Acc: 9.19%\n",
      "Round 3/20 | Test Loss: 2.2992 | Test Acc: 9.68%\n",
      "Round 4/20 | Test Loss: 2.2973 | Test Acc: 10.12%\n",
      "Round 5/20 | Test Loss: 2.2954 | Test Acc: 10.68%\n",
      "Round 6/20 | Test Loss: 2.2936 | Test Acc: 11.36%\n",
      "Round 7/20 | Test Loss: 2.2918 | Test Acc: 11.83%\n",
      "Round 8/20 | Test Loss: 2.2900 | Test Acc: 12.59%\n",
      "Round 9/20 | Test Loss: 2.2883 | Test Acc: 13.05%\n",
      "Round 10/20 | Test Loss: 2.2866 | Test Acc: 13.49%\n",
      "Round 11/20 | Test Loss: 2.2849 | Test Acc: 13.85%\n",
      "Round 12/20 | Test Loss: 2.2832 | Test Acc: 14.32%\n",
      "Round 13/20 | Test Loss: 2.2815 | Test Acc: 14.63%\n",
      "Round 14/20 | Test Loss: 2.2798 | Test Acc: 14.97%\n",
      "Round 15/20 | Test Loss: 2.2781 | Test Acc: 15.25%\n",
      "Round 16/20 | Test Loss: 2.2765 | Test Acc: 15.54%\n",
      "Round 17/20 | Test Loss: 2.2748 | Test Acc: 15.67%\n",
      "Round 18/20 | Test Loss: 2.2731 | Test Acc: 15.91%\n",
      "Round 19/20 | Test Loss: 2.2715 | Test Acc: 16.12%\n",
      "Round 20/20 | Test Loss: 2.2698 | Test Acc: 16.41%\n",
      "--- FedSGD Training Complete ---\n",
      "\n",
      "--- Starting Centralized Training ---\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 6.87 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 3. Run Training\u001b[39;00m\n\u001b[0;32m     13\u001b[0m history_fedsgd \u001b[38;5;241m=\u001b[39m train_fedsgd(\n\u001b[0;32m     14\u001b[0m     model_fedsgd, fedsgd_loaders, test_loader, client_weights, LEARNING_RATE, NUM_ROUNDS\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m history_centralized \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_centralized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_centralized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentral_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_ROUNDS\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 4. Analyze and Verify Equivalence\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Verification of Equivalence ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mtrain_centralized\u001b[1;34m(model, data_loader, test_loader, lr, num_rounds)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Perform one full-batch SGD step\u001b[39;00m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, full_target)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\OneDrive - Higher Education Commission\\Fall 2026\\CS 6304 - ATML\\PA4\\ATML_PA4\\FedSGD_vs_CentralisedSGD\\federated_learning.py:67\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Conv block 1\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Conv block 2\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 6.87 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# --- Main Comparison Logic ---\n",
    "\n",
    "# 1. Load Data\n",
    "central_loader, fedsgd_loaders, test_loader, client_weights = load_data_task1(NUM_CLIENTS)\n",
    "\n",
    "# 2. Initialize Models\n",
    "# Must start with the *exact* same weights\n",
    "model_fedsgd = SimpleCNN()\n",
    "model_centralized = SimpleCNN()\n",
    "model_centralized.load_state_dict(copy.deepcopy(model_fedsgd.state_dict()))\n",
    "\n",
    "# 3. Run Training\n",
    "history_fedsgd = train_fedsgd(\n",
    "    model_fedsgd, fedsgd_loaders, test_loader, client_weights, LEARNING_RATE, NUM_ROUNDS\n",
    ")\n",
    "history_centralized = train_centralized(\n",
    "    model_centralized, central_loader, test_loader, LEARNING_RATE, NUM_ROUNDS\n",
    ")\n",
    "\n",
    "# 4. Analyze and Verify Equivalence\n",
    "print(\"\\n--- Verification of Equivalence ---\")\n",
    "print(\"Round | FedSGD Acc (%) | Central Acc (%) | Norm of Weight Difference\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "weight_diffs = []\n",
    "rounds = range(1, NUM_ROUNDS + 1)\n",
    "\n",
    "for i in range(NUM_ROUNDS):\n",
    "    acc_f = history_fedsgd['test_acc'][i]\n",
    "    acc_c = history_centralized['test_acc'][i]\n",
    "    \n",
    "    # Get weights from this round\n",
    "    params_f = history_fedsgd['weights'][i]\n",
    "    params_c = history_centralized['weights'][i]\n",
    "    \n",
    "    # Convert to vectors for comparison\n",
    "    vec_f = nn.utils.parameters_to_vector([p for p in params_f.values()])\n",
    "    vec_c = nn.utils.parameters_to_vector([p for p in params_c.values()])\n",
    "    \n",
    "    # Calculate L2 norm of the difference\n",
    "    diff = torch.norm(vec_f - vec_c, p=2).item()\n",
    "    weight_diffs.append(diff)\n",
    "    \n",
    "    print(f\"  {i+1:2d}  |    {acc_f:6.2f}    |    {acc_c:6.2f}     | {diff:e}\")\n",
    "    \n",
    "# 5. Plot Results\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot 1: Test Accuracy Comparison\n",
    "ax1.plot(rounds, history_fedsgd['test_acc'], 'bo-', label='FedSGD', markersize=4)\n",
    "ax1.plot(rounds, history_centralized['test_acc'], 'rs--', label='Centralized SGD', markersize=4)\n",
    "ax1.set_title(f'Task 1: FedSGD vs. Centralized SGD (IID, Full-Batch, {NUM_CLIENTS} Clients)')\n",
    "ax1.set_xlabel('Communication Round / SGD Step')\n",
    "ax1.set_ylabel('Test Accuracy (%)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Norm of Weight Difference\n",
    "ax2.plot(rounds, weight_diffs, 'g-o', label='L2 Norm( || W_fedsgd - W_central || )', markersize=4)\n",
    "ax2.set_title('Difference Between Model Weights')\n",
    "ax2.set_xlabel('Communication Round / SGD Step')\n",
    "ax2.set_ylabel('L2 Norm')\n",
    "ax2.set_yscale('log') # Use log scale to see small differences\n",
    "ax2.legend()\n",
    "ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task1_comparison.png', dpi=300)\n",
    "print(\"\\nSaved comparison plot to 'task1_comparison.png'\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
